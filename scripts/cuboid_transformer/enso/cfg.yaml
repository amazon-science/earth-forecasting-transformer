dataset:
  in_len: 12
  out_len: 14
  nino_window_t: 3
  in_stride: 1
  out_stride: 1
  train_samples_gap: 1
  eval_samples_gap: 1
  normalize_sst: true
layout:
  in_len: 12
  out_len: 14
  layout: "NTHWC"
optim:
  total_batch_size: 64
  micro_batch_size: 16
  seed: 0
  method: "adamw"
  lr: 0.0001
  wd: 1.0e-05
  gradient_clip_val: 1.0
  max_epochs: 100
  # scheduler
  lr_scheduler_mode: "cosine"
  min_lr_ratio: 1.0e-3
  warmup_min_lr_ratio: 0.0
  warmup_percentage: 0.2
  # early stopping
  early_stop: true
  early_stop_mode: "min"
  early_stop_patience: 5
  save_top_k: 5
logging:
  logging_prefix: "Cuboid_ENSO"
  monitor_lr: true
  monitor_device: false
  track_grad_norm: -1
  use_wandb: false
trainer:
  check_val_every_n_epoch: 5
  log_step_ratio: 0.001
  precision: 32
vis:
  train_example_data_idx_list: [0, ]
  val_example_data_idx_list: [0, ]
  test_example_data_idx_list: [0, ]
  eval_example_only: false
model:
  input_shape: [12, 24, 48, 1]
  target_shape: [14, 24, 48, 1]
  base_units: 64
  # block_units: null
  scale_alpha: 1.0

  enc_depth: [1, 1]
  dec_depth: [1, 1]
  enc_use_inter_ffn: true
  dec_use_inter_ffn: true
  dec_hierarchical_pos_embed: true

  downsample: 2
  downsample_type: "patch_merge"
  upsample_type: "upsample"

  num_global_vectors: 0
  use_dec_self_global: false
  dec_self_update_global: true
  use_dec_cross_global: false
  use_global_vector_ffn: false
  use_global_self_attn: false
  separate_global_qkv: false
  global_dim_ratio: 1

  self_pattern: "axial"
  cross_self_pattern: "axial"
  cross_pattern: "cross_1x1"
  cross_last_n_frames: null

  attn_drop: 0.1
  proj_drop: 0.1
  ffn_drop: 0.1
  num_heads: 4

  ffn_activation: "gelu"
  gated_ffn: false
  norm_layer: "layer_norm"
  padding_type: "zeros"
  pos_embed_type: "t+h+w"
  use_relative_pos: true
  self_attn_use_final_proj: true
  dec_use_first_self_attn: false

  z_init_method: "zeros"
  initial_downsample_type: "conv"
  initial_downsample_activation: "leaky"
  initial_downsample_scale: [1, 1, 2]
  initial_final_sample_num_conv: 2
  checkpoint_level: 0

  attn_linear_init_mode: "0"
  ffn_linear_init_mode: "0"
  conv_init_mode: "0"
  down_up_linear_init_mode: "0"
  norm_init_mode: "0"
